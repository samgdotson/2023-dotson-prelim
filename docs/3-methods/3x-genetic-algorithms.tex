\section{Genetic Algorithms}
\label{section:genetic-algorithms}

Rather than rely on \ac{lp} to model future capacity requirements, in this thesis, \acp{ga} assume the role of investment optimizer. \acp{ga} share a fundamental 
algorithmic structure, which is \cite{blank_pymoo_2020}
\begin{enumerate}
    \item \textbf{Initialize} a starting population of $N_p$ individuals, where each 
    individual has a set of ``genes'' that are randomly chosen from the bounds of the 
    decision variables.
    \item Each individual in the population is \textbf{evaluated} for ``fitness.'' 
    \item The \textbf{fittest}, $N_f$ individuals ``survive'' and persist in the next generation.
    \item A ``selection'' operator \textbf{chooses} among the surviving individuals to mate.
    \item The parents are \textbf{combined} using a ``crossover'' operator, thereby filling the remaining $N_p - N_f$ individuals for the next generation.
    \item The offspring are finally \textbf{mutated} with some 
    probability, $\mu$, to improve genetic diversity.
\end{enumerate}
\noindent
Figure \ref{fig:genetic-alg} illustrates the flow of these steps applied to
an energy systems model.

\begin{figure}[ht]
        \centering
        \begin{tikzpicture}[node distance=1.7cm]
                \tikzstyle{every node}=[font=\small]
                \node (1) [lbblock] {\textbf{Create initial population\\ of capacity sets}};
                \node (2) [lbblock, below of=1] {\textbf{Evaluate dispatch model and calculate objectives}};
                \node (3) [lbblock, below of=2] {\textbf{Survival}}; 
                \node (4) [lbblock, below of=3] {\textbf{Selection}};
                \node (5) [lbblock, below of=4] {\textbf{Crossover}};
                \node (6) [lbblock, below of=5] {\textbf{Mutation}};
                \node (7) [lbblock, below of=6] {\textbf{Is the termination \\ criteria satisfied?}};
                \node (8) [loblock, below of=7] {\textbf{Done}};
                \draw [arrow] (1) -- (2);
                \draw [arrow] (2) -- (3);
                \draw [arrow] (3) -- (4);
                \draw [arrow] (4) -- (5);
                \draw [arrow] (5) -- (6);
                \draw [arrow] (6) -- (7);
                \draw [arrow] (7) -- (8);
                \draw [arrow] (7) -- node[anchor=east] {yes} (8);
                \draw [arrow] (7) -- ([shift={(0.5cm,0cm)}]7.east)-- node[anchor=west] {no} ([shift={(0.5cm,0cm)}]2.east)--(2);
        \end{tikzpicture}
        \caption{The basic flow of the \ac{ga} used in this thesis.}
        \label{fig:genetic-alg}
\end{figure}

\subsection{Specific \Aclp{ga}}
The variety of \acp{ga} comes from different types of operators being applied to 
the selection, crossover, and mutation steps. Section \ref{section:moo-in-energy} 
showed that \ac{nsga2} is a popular genetic algorithm choice. However, this algorithm
performs poorly with greater than three objectives \cite{deb_fast_2002, seada_unified_2016}. In this thesis, I use a more modern algorithm, 
\ac{unsga3}. \ac{unsga3} builds on its predecessors \ac{nsga2} and \ac{nsga3} by unifying
efficient solutions of mono-, multi-, and many-objective problems in a single algorithm.


\ac{nsga2} improves on the basic \ac{ga} by introducing a more sophisticated mating and 
selection algorithms. Instead of random selection, the individuals are sorted by rank 
(i.e. fitness) and crowding distance in binary tournament mating selection. The crowding 
distance is simply the Manhattan distance between individuals. A greater crowding 
distance is desirable to preserve diversity and since the extreme points are maximally 
diverse they should always persist and are therefore assigned a crowding distance of 
infinity \cite{deb_fast_2002}.

The successor to \ac{nsga2}, \ac{nsga3}, enhances the many-objective capabilities of 
the former by introducing reference directions. Reference directions are used for 
initialization and the survival steps. In addition to fitness, individuals are chosen 
based on their proximity to a reference line, thus ensuring population diversity which
greatly important for many-objective problems. Since diversity is handled by reference
directions, individuals are selected randomly for mating. References directions are 
rays passing through uniformly spaced points on the unit simplex 
\cite{seada_unified_2016, blank_generating_2021}. In this thesis, I use the Riesz s-
Energy method described by Blank et al. to calculate these points for a problem with an 
arbitrary number of objectives \cite{blank_generating_2021}. Figure \ref{fig:ref-dirs} 
illustrates a set of initialized reference directions.

\begin{figure}[h]
  \centering
  \resizebox{0.6\columnwidth}{!}{\input{figures/reference_directions.pgf}}
  \caption{A set of reference directions for a three-objective problem.}
  \label{fig:ref-dirs}
\end{figure}

\ac{nsga2} is useful for mono- and multi-objective functions while \ac{nsga3} is better
for many-objective problems. \ac{unsga3} can handle any number of objectives by 
introducing the binary tournament from \ac{nsga2} and reducing to the most efficient 
algorithm for the problem at hand \cite{seada_unified_2016}. Chapter \ref{chapter:benchmark-results} demonstrates these three algorithms in 

\subsection{Hyperparameter Tuning}
Similar to other machine learning models, \acp{ga} have several hyperparameters that
must be tuned for optimal behavior. These hyperparameters include probabilities for mutation, crossover, and selection, as well as the number of parents, number of offspring, and population size. Determining ideal hyperparameters is often 
performed using either a grid search or random sampling \cite{bergstra_random_2012}. 
This thesis adopts the approach from Blank and Deb \cite{blank_pymoo_2020} using
a genetic algorithm to identify the ideal hyperparameters. A problem is converted into a single objective problem using the desired algorithm, then a second genetic algorithm drives the problem where the decision variables are hyperparameters of the desired algorithm.

\subsection{Convergence}
There are several ways to stop a simulation in \ac{pymoo}. A simulation may end after reaching
\begin{enumerate}
    \item a specified end time (e.g., 100 minutes),
    \label{it:convergence1}
    \item a specified number of evaluations or iterations (e.g., 500 individual evaluations or 20 generations),
    \label{it:convergence2}
    \item a tolerance value in the design space,
    \label{it:convergence3}    
    \item a tolerance value in the objective space.
    \label{it:convergence4}
\end{enumerate}

It is possible that criteria \ref{it:convergence3} and \ref{it:convergence4} will never be met; therefore, they are often
combined with either of the first two criteria. The fourth convergence criterion is the most interesting due to the challenge of calculating an appropriate metric. This thesis uses the weakly Pareto-compliant algorithm \ac{igdp} over the more common hypervolume 
calculation due to its reduced computational requirements \cite{ishibuchi_modified_2015}.

\subsection{\acs{pymoo} and \acs{deap}}

The \ac{esom} framework developed in this thesis is built on top of \ac{pymoo} and \ac{deap}. 
\ac{pymoo} is an open-source library for \acp{ga} developed by the creators of 
\ac{nsga2} and \ac{unsga3} \cite{blank_pymoo_2020}. This package implements several 
\acp{ga} out-of-the-box and offers a set of visualization tools and hyperparameter tuning. \ac{deap} is another open-source library offering a toolkit for constructing \acp{ga} and therefore has fewer prepackaged algorithms than \ac{pymoo}. There are robust reasons to use
both libraries, so \ac{osier} facilitates both.




